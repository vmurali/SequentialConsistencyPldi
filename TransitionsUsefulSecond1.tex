\section{Specifying Sequential Consistency}\label{sec:sc}

\begin{figure}
\small
\centering
\begin{boxedminipage}[c]{.48\textwidth}
%\centering
\inference
[Halt]
{\theta(i) = (s, \pc) & \dec(s,\pc) = H}
{\io{SC}{\theta,m}{H}{}}

\inference
[NonMem]
{\theta(i) = (s, \pc) & \dec(s,\pc) = (\nm, x) \\ \exec(s, \pc, (\nm, x)) = (\pc', s')}
{\io{SC}{\theta,m}{\theta[i \coloneqq (s', \pc')], m}{}}

\inference
[Load]
{\theta(i) = (s,\pc) & \dec(s,\pc) = (\ld, x, a) \\ \exec(s, \pc, (\ld, x, m(a))) = (\pc', s')}
{\io{SC}{\theta,m}{\theta[i\coloneqq(s',\pc')],m}{}}

\inference
[Store]
{\theta(i) = (s,\pc) & \dec(s,\pc) = (\st, a, v) \\ \exec(s, \pc, (\st)) = (\pc', s')}
{\io{SC}{\theta,m}{\theta[i\coloneqq(s',\pc')],m[a\coloneqq v]}{}}
\end{boxedminipage}
\caption{LTS for SC with $n$ simple processors}
\label{Ref}
\end{figure}

Our final theorem in this paper establishes that a particular complex hardware
system implements sequential consistency (SC) properly.  We state the theorem
in terms of the trace refinement relation $\sqsubseteq$ developed in the prior
section.  Therefore, we need to define an LTS embodying SC.  The simpler this
system, the better.  We do not need to worry about its performance properties,
since we will prove that an optimized system remains faithful to it.

Figure~\ref{Ref} defines an $n$-processor, sequentially consistent system as an
LTS.  The definition is parametrized over details of an \emph{instruction set
architecture} (ISA).  In particular, the ISA gives us some domains of
architectural states $s$ (e.g., register files) and of program counters $\pc$.
A function $\dec(s, \pc)$ figures out which instruction $\pc$ references in the
current state, returning the instruction's ``decoded'' form.  A companion
function $\exec(s, \pc, \mathit{dec})$ actually executes the instruction,
returning the next program counter $\pc'$ and the new state $s'$.

The legal instruction forms, which are outputs of $\dec$, are $(\nm, x)$, for
an operation not accessing memory; $(\ld, x, a)$, for a memory load from
address $a$; $(\st, a, v)$, for a memory store of value $v$ to address $a$; and
$H$, for a ``halt'' instruction that moves the LTS to state $H$. The parameter
$x$ above represents the rest of the instruction, including the
opcode, registers, constants, \etc{}

The legal inputs to $\exec$ encode both a decoded instruction and any relevant
responses from the memory system.  These inputs are $(\nm, x)$ and $\st$, which
need no extra input from the memory; and $(\ld, x, v)$, where $v$ gives the
contents of the requested memory cell.

We define the initial state of SC as $(\theta_0, m_0)$, where $m_0$ is
some initial memory fixed throughout our development, mapping every
address to value $v_0$; and $\theta_0$ maps every processor ID to
$(s_0, \pc_0)$, using architecture-specific default values $s_0$ and
$\pc_0$.

\medskip

This LTS encodes Lamport's notion of SC, where processors take turns executing
nondeterministically in a simple interleaving.  We need know very few details
of the ISA to define the SC model.  Our final, optimized system is parametrized
over an ISA in the same way.  In the course of the rest of this paper, we will
define an optimized system $O$ and prove $O \sqsubseteq \text{SC}$.

However, we are not interested merely in verifying the particular system $O$.
Instead, we want to structure our proof \emph{modularly}, so that different
common hardware components can be verified separately, and, in a black-box
manner, we may assemble a correctness proof for any composition of these
components.  For instance, we want to verify CPUs and memory systems
separately, picking any realization of each to produce a verified system, with
minimal new proof effort. As a more concrete example, memory systems should be
verifiable independently of which weak memory model processors assume; we want
to reuse memory proofs for different kinds of processors.

To support this style of modular decomposition, we need to introduce a few
intermediate systems.

Note that, in this setting, an operational specification like the LTS
for SC is precisely the right characterization of \textbf{full functional
  correctness} for a hardware design, much as a
precondition-postcondition pair does that job in a partial-correctness
Hoare logic.  Our SC LTS fully constrains observable behavior of a
system to remain consistent with simple interleaving.  Similar
operational models are possible as top-level specifications for
systems following weaker memory models, by giving the LTS for the \emph{Local
Buffer} component and composing the three components simultaneously.

\begin{figure}
\small
\centering
\begin{boxedminipage}[c]{.48\textwidth}
\inference
[Halt]
{\dec(s,\pc) = H}
{\io{P$_\text{ref}$}{s,\pc,\bot}{H}{}}

\inference
[NM]
{\dec(s,\pc) = (\nm, x) & \exec(s, \pc, (\nm, x)) = (\pc', s')}
{\ioe{P$_\text{ref}$}{s,\pc,\bot}{s', \pc', \bot}}

\inference
[LdRq]
{\dec(s,\pc) = (\ld, x, a)}
{\ior{P$_\text{ref}$}{s, \pc, \bot}
{s, \pc, \top}{\epsilon, \ld, a}}

\inference
[StRq]
{\dec(s,\pc) = (\st, a, v)}
{\ior{P$_\text{ref}$}{s, \pc, \bot}
{s, \pc, \top}{\st, a, v}}

\inference
[LdRp]
{\dec(s,\pc) = (\ld, x, a) & \exec(s, \pc, (\ld, x, v)) = (\pc', s')}
{\iol{P$_\text{ref}$}{s, \pc, \top}
{s', \pc', \bot}{\epsilon, \ld, v}}

\inference
[StRp]
{\dec(s,\pc) = (\st, a, v) & \exec(s, \pc, (\st)) = (\pc', s')}
{\iol{P$_\text{ref}$}{s, \pc, \top}
{s', \pc', \bot}{\st}}
\end{boxedminipage}

\caption{LTS for a simple decoupled processor (P$_{\text{ref}}$)}
\label{Pref$}
\end{figure}


\section{Respecifying Sequential Consistency with Communication}\label{sec:ref}

Realistic hardware systems do not implement the monolithic SC model of
Figure~\ref{Ref} directly.  Instead, there is usually a split between
processors and memory. Here we formalize that split using LTSes that 
compose to produce a system refining the simple SC model.

Figure~\ref{Pref$} defines an LTS for a simple \emph{decoupled} processor
(P$_\text{ref}$).  That is, memory does not appear within a processor's state, but
instead, to load from or store to an address, we send \emph{requests} to the
memory system and receive its \emph{responses}.  Both kinds of messages are
encoded as labels, $\text{ToM}$ for requests to memory and $\text{ToP}$ for
responses from memory back to the processor.

A state of P$_{\text{ref}}$ is a triple $(s, \pc, \wait)$, giving the current architectural
state $s$ and program counter $\pc$, as well as a Boolean flag $\wait$
indicating whether the processor is blocked waiting for a response from the
memory system. As in the SC model, we change the state of the processor to $H$
whenever $\dec$ returns $H$. 
%Note that there is some subtlety in the design of
%this semantics regarding halting; we plan ahead to be sure we will be able to prove the
%appropriate simulation facts.  Those proofs would be harder if, \eg{}
%a processor could halt while it is still waiting for a memory response.

As initial state for system $P_\text{ref}$, we use $(s_0, \pc_0, \bot)$.

%\medskip

The simple memory defined earlier in Figure~\ref{$M_m$} is meant to be composed with P$_{\text{ref}}$
processors.  %The input-output interface of this memory $M_m$ is largely what
%one would expect from the P$_{\text{ref}}$ definition. 
%However, in anticipation of the
%more complex processor we will define later, we add requests and responses for
%\emph{speculative loads}.  These are load operations that a processor may
a%ttempt, before it knows that the program will actually need the result.  Many
s%peculative loads from a single processor may be in flight at once, improving
p%erformance via parallelism. 
A request to memory like $(t, \ld, a)$ asks
for the value of memory cell $a$, associating a \emph{tag} $t$ that the
processor can use to match responses to requests.  Those responses take the
form $(t, \ld, v)$, giving the value $v$ of the requested memory address.

A memory state is a triple $(\inp, \outp, m)$, giving not just the
memory $m$ itself, but also buffers $\inp$ and $\outp$ for receiving
processor requests and batching up responses to processors,
respectively.  We define the initial state of the $M_m$ LTS as
$(\emptyset, \emptyset, m_0)$, with empty queues.

%Note that we have used $\uplus$ for inserting (or removing) from the buffers
%$\inp$ and $\outp$. The intent is to convey that $\inp$ and $\outp$ are treated
%as unordered sets. We will later use $\coloncolon$ for inserting (or removing)
%from FIFO buffers. Intuitively, the unordered requests/responses set suffices
%even for requests/responses for the same address because the processor always
%waits for a response before sending the next request.
%\medskip

Now we can compose these LTSes to produce an implementation of SC. 
% Rule SpLoad
%in Figure~\ref{$M_m$} will be irrelevant in this particular composition, since
%P$_{\text{ref}}$ never does speculation.

For a system of $n$ processors, our decoupled SC implementation is
$\text{P$^n_{\text{ref}}$} + M_m$.

\begin{figure}[h]
\begin{boxedminipage}{\columnwidth}
\small
$\upd((s, \pc, \_), \mathit{rs}) =$

\begin{math}
\;\;\;\;\;\left\{
\begin{array}{ll}
s', \pc' &: (\ld, v) \in \mathit{rs} \wedge \dec(pc, s) = (\ld, x, a) \wedge\\
\multicolumn{2}{r}{\exec(pc, s, (\ld, x, v)) = (\pc', s'))}\\
s', \pc' &: (\st) \in \mathit{rs} \wedge \dec(pc, s) = (\st, a, v) \wedge\\
\multicolumn{2}{r}{\exec(pc, s, (\st)) = (\pc', s')}\\
s, \pc &: \text{otherwise if } \dec(pc,s) \neq H\\
H &: \text{otherwise}
\end{array}
\right.
\end{math}
\end{boxedminipage}
\caption{Mapping of states from $(\text{P$^n_\text{ref}$} + M_m)$ to SC}
\label{smap}
\end{figure}

\begin{theorem}
$\text{P$^n_{\text{ref}}$} + M_m \sqsubseteq SC$\label{scthm}
\end{theorem}
\begin{prf}
By induction on traces of the decoupled system.  We need to choose an
abstraction function $f$ from states of the complex system to states of
the simple system.  This function must be inductive in the
appropriate sense: a step from $s$ to $s'$ on the left of the
simulation relation must be matched by sequences of steps on the right
from $f(s)$ to $f(s')$.

Figure~\ref{smap} defines a suitable function $\upd$ applying to the
state of a single processor, and our abstraction function is the
natural lifting to $n$ processors.
The single-processor version $\upd$ maps
the decoupled state of a processor to an SC state, when also given the
set $rs$ of memory responses that have not been processed yet. If such a
response exists, then the updated $s', \pc'$ is the result of
receiving the response and performing the state updates on the current $s,
\pc$ pair.  We can also prove that multiple responses will not be present in
the $rs$ set at any point, intuitively because the processor goes into a wait
state as soon as it sends a request, and the single response, if present, will
correspond exactly to the instruction currently waiting for the response.
Therefore, the apparently ambiguous definition of $\upd$ is actually
well-founded.  The only interesting case in the proof arises when the memory adds a
response to $rs$; one has to show that the SC system can lead to the same
(projected) state.
\end{prf}

\section{Speculative Out-Of-Order Processor}\label{sec:ooo}

The previous section divided a multiprocessor system into two rather
simple components, processors and memories.  We are now ready to begin
developing more complex, optimized versions of each piece, starting
with processors.

We implement a \emph{speculative} processor, which may create many
simultaneous outstanding requests to the memory, as an optimization to
increase parallelism.  Our processor proof is in some sense generic
over correct speculation strategies.  We parametrize over two key
components of a processor design: a \emph{branch predictor}, which
makes guesses about processor-local control flow in advance of
%receiving enough memory responses to resolve
resolving conditional jumps; and a
\emph{reorder buffer}, which decides what speculative instructions (like memory loads)
are worth issuing at which moments (in effect \emph{reordering} later
instructions to happen before earlier instructions have finished).

The branch predictor is the simpler of the two components, whose state
is indicated with metavariable $\ppc$.
The operations on such state are $\curPpc(\ppc)$, to extract the
current program-counter prediction; $\nextPpc(\ppc)$, to get the predicted
program-counter for the next instruction; and $\setBp(\ppc, \pc)$,
to reset prediction to begin at a known-accurate position $\pc$. We need to impose no explicit correctness criterion on
branch predictors; the processor uses predictions only as hints, and
it always resets the predictor using $\setBp$ after detecting an
inaccurate hint.

The interface and formal contract of a reorder buffer are more
involved.  We write $\rob$ as a metavariable for reorder buffer
state. $\phi$ denotes the state of an empty buffer. The operations associated with $\rob$ are:
\begin{itemize}
%\item $\phi$, the state of an empty buffer.
\item $\add(\pc, \rob)$, which appends the program instruction at location $\pc$ to the list of instructions that the buffer is allowed to consider executing.
\item $\compute(\rob)$, which models a
step of computation inside the buffer, returning both an updated state
and an optional speculative load to issue. For instance, it invokes the $\dec$
and $\exec$ functions (as defined for SC) internally to obtain the next program
counter, state, \etc{} (but the actual states are not updated).
\item $\upd\ld(\rob,t,v)$, which informs the buffer that the memory
has returned result value $v$ for the speculative load with tag $t \neq \epsilon$.
\item $\commit(\rob)$, which returns the next instruction in serial
program order, if we have accumulated enough memory responses to execute it
accurately, or returns
$\epsilon$ otherwise.  When $\commit$ returns an instruction, it also
returns the associated program counter plus the next program counter
we would advance to afterward.
 Furthermore, the instruction is
extended with any relevant response from memory (used only for load
instructions, obtained through $\upd\ld$) and with the new architectural state (\eg{} register
file) after execution.
\item $\retire(\rob)$, which informs the buffer that its $\commit$
instruction was executed successfully, so it is time to move on to the
next instruction.
\end{itemize}

We postpone characterization of correct reorder buffers until after we
present the LTS for speculating processors.

\begin{figure}[t]
\small
\centering
\begin{boxedminipage}[c]{.95\textwidth}
\inference[Fetch]
{}
{\ioxe{P$_\text{so}$}{\hspace{-.2cm}s,\pc,\wait,\\\rob,\ppc}{\hspace{-.2cm}s,\pc,\wait,\add(\curPpc(\ppc),\\\;\;\;\rob),\nextPpc(\ppc)}}

\inference[Compute]
{\compute(\rob) = (\rob',\epsilon)}
{\ioe{\text{P$_\text{so}$}}{s,\pc,\wait,\rob,\ppc}{s,\pc,\wait,\rob',\ppc}}

\inference[SpLdRq]
{\compute(\rob) = (\rob',(\spc\ld,t,a))}
{\ior{\text{P$_\text{so}$}}{s,\pc,\wait,\rob,\ppc}{s,\pc,\wait,\rob',\ppc}
{t,\ld, a}}

\inference[SpLdRp]
{t \neq \epsilon}
{\iox{\text{P$_\text{so}$}}{s,\pc,\wait,\\\rob,\ppc}
{s,\pc,\wait,\\\upd\ld(\rob, t, v),\ppc
}{\text{ToP}(t,\ld,v)}}

\inference[Abort]
{\commit(\rob) = (\pc',\_,\_) & \pc' \neq \pc}
{\ioxe{\text{P$_\text{so}$}}{s,\pc,\wait,\\\rob,\ppc}{s,\pc,\wait,\phi,\\\setBp(\ppc,\pc)}}

\inference[Halt]
{\commit(\rob) = H}
{\ioe{\text{P$_\text{so}$}}{s,\pc,\bot,\rob,\ppc}{H}}

\inference[Nm]
{\commit(\rob) = (\pc,\pc',(\nm,s'))}
{\ioe{\text{P$_\text{so}$}}{s,\pc,\bot,\rob,\ppc}{s',\pc',\bot,\retire(\rob),\ppc}}

\inference[StRq]
{\commit(\rob) = (\pc,\pc',(\st,a,v,s'))}
{\ior{\text{P$_\text{so}$}}{s,\pc,\bot,\rob,\ppc}{s,\pc,\top,\rob,\ppc}
{\st, a, v}}

\inference[LdRq]
{\commit(\rob) = (\pc,\pc',(\ld,x,a,v,s'))}
{\ior{\text{P$_\text{so}$}}{s,\pc,\bot,\rob,\ppc}{s,\pc,\top,\rob,\ppc}{\epsilon, \ld, a}}
\inference[StRp]
{\commit(\rob) = (\pc,\pc',(\st,a,v,s'))}
{\iol{\text{P$_\text{so}$}}{s,\pc,\top,\rob,\ppc}{s',\pc',\bot,\retire(\rob),\ppc}{\st}}

\inference[LdRpGood]
{\commit(\rob) = (\pc,\pc',(\ld,x,a,v,s'))}
{\iox{\text{P$_\text{so}$}}{s,\pc,\top,\\\rob,\ppc}{s',\pc',\bot,\\\retire(\rob),\ppc}{\text{ToP}(\epsilon, \ld,v)}}

\inference[LdRpBad]
{\commit(\rob) = (\pc,\pc',(\ld,x,a,v',s')) \\ v' \neq v & \exec(s,\pc,(\ld,x,v)) = (\pc'', s'')}
{\iox{\text{P$_\text{so}$}}{s,\pc,\top,\\\rob,\ppc}{s'',\pc'',\bot,\phi,\\\setBp(\ppc,\pc)}{\text{ToP}(\epsilon, \ld, v)}}


\end{boxedminipage}

\caption{Speculating, out-of-order issue processor}
\label{Ooo}
\end{figure}

Figure~\ref{Ooo} defines that LTS,
P$_{\texttt{so}}$. This processor may issue arbitrary speculative
loads, but it only ever \emph{commits} the next instruction in serial
program order. 
We will see the processor issue two kinds of loads, a speculative
load (whose tag is not $\epsilon$) and a commit or real load (whose tag is $\epsilon$).  To maintain SC, every speculative
load must have a matching verification load later on, and we maintain
the illusion that the program only depends on the results of
verification loads, which, along with stores, \emph{must be issued in
serial program order}.

When committing a
previously issued speculative load instruction, the associated speculative
memory load response is verified against the new commit load response. If the
resulting values do not match, the processor terminates all past uncommitted
speculation, by emptying the reorder buffer and resetting the
next predicted program counter in the branch predictor to the correct next value. This step mimics
the mechanism implemented in most modern-day processors, operating on the
conservative assumption that the processor may have performed arbitrary
incorrect speculation after receiving the inaccurate response from
memory.  (Note that here ``inaccurate'' does not mean ``wrong at
the time the response was received,'' but rather ``wrong at commit
time'' or ``correct before, but not anymore.'')

A full processor state is $(s, \pc, \wait, \rob, \ppc)$, comprising
architectural state, the program counter, a Boolean flag indicating
whether the processor is waiting for a memory response about an
instruction being committed, and the reorder buffer and branch
predictor states. Its initial state is given by $(s_0, \pc_0, \bot, \phi, \ppc_0)$.
The interface of this processor with memory (\ie{}
communication labels with $\toM, \toP$) is identical
to that of the reference processor.
% but now we can exercise a memory
%system's transitions for speculative loads.

%The processor state is represented in the form $(s,\pc,\wait,\rob,\ppc)$, where
%the extra states compared to the decoupled processor of Figure~\ref{Pref$}. The
%additional states $\rob$ and $\ppc$ correspond to the reorder buffer holding
%the speculative operations and the speculative program counter indicating the
%next instruction to execute speculatively. 

\begin{figure*}[t]
\begin{inv}
If $P_\text{so}$ reaches a state $(s, \pc, \wait, \rob, \ppc)$, \ie{}\hspace{.2cm}
$\io{P$_\text{so}^{*}$}{s_0,\pc_0,\bot,\phi,\ppc_0}{s,\pc,\wait,\rob,\ppc}{\eta} \Rightarrow$

\begin{math}
\small
\;\;\;\;\;\;\;\;\left\{
\begin{array}{ll}
\commit(\rob) = (\pc, \pc', (\nm, s')) &\Rightarrow
\exists x. \; \dec(s,\pc) = (\nm, x) \wedge \exec(s,\pc, (\nm,x)) =
(\pc', s')\\
\commit(\rob) = (\pc, \pc', (\ld, x, a, v, s')) & \Rightarrow
\dec(s,\pc) = (\ld, x,a) \wedge \exec(s,\pc, (\ld,x,v)) = (\pc', s')\\
\commit(\rob) = (\pc, \pc', (\st, a, v, s')) &\Rightarrow
\dec(s,\pc) = (\st, a,v) \wedge \exec(s,\pc, (\st)) =
(\pc', s')\\
\commit(\rob) = H & \Rightarrow
\dec(s, \pc) = H\\
\end{array}
\right.\end{math}
\label{rob}
\end{inv}
\vspace{-.5cm}
\caption{Correctness of reorder buffer}
\label{robfig}
\end{figure*}

Finally, we impose a general correctness condition on reorder
buffers.  They must maintain Invariant~\ref{rob} in Figure~\ref{robfig}.
Intuitively, whenever the buffer claims in a $\commit$
output that a particular instruction is next to execute, causing
certain state changes, that instruction must really be next in line according
to how the program runs in the SC system, and running it must really cause
those state changes.

When this condition holds, we may conclude the correctness theorem for
out-of-order processors.  We use a trace-transformation function
$\noSpec$ that drops all speculative-load requests and responses (\ie{} those load requests and responses whose tags are $\epsilon$).
See Definition~\ref{refines} for a review of how we use such
functions in framing trace refinement.  Intuitively, we prove that any
behavior by the speculating processor can be matched by the simple
processor, with speculative messages erased.
%\vspace{-.6cm}
\begin{theorem}
\label{ocorrect}
$\text{P$_\text{so}$} \sqsubseteq_\noSpec$ P$_\text{ref}$
\end{theorem}
\begin{prf}
By induction on P$_\text{so}$ traces, using a proper abstraction
function (which drops the speculative messages and the $\rob$ and $\ppc$ states) to relate the two systems.  Invariant~\ref{rob} is crucial to
relate the reorder buffer's behavior with the simple in-order
execution of P$_\text{ref}$.
\end{prf}

\begin{corollary}
\label{ges}
$\text{P$^n_\text{so}$} \sqsubseteq_{\noSpec^n}$ P$_\text{ref}^n$
\end{corollary}
\begin{prf}
Direct consequence of Theorems~\ref{ocorrect},~\ref{liftn} (the
latter is one of our preliminary results about $n$-repetition).
\end{prf}
