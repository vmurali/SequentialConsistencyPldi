===========================================================================
                           PLDI '15 Review #11A
                     Updated 9 Dec 2014 7:29:02am EST
---------------------------------------------------------------------------
   Paper #11: Modular Deductive Verification of Multiprocessor Hardware
              Designs
---------------------------------------------------------------------------

                      Overall merit: 2. Accept
                         Conviction: 2. Low

                         ===== Paper summary =====

The paper presents a modular verification framework for hardware designs, and applies it to an SC multicore system. The processors are assumed to implement speculation and concurrent execution of loads; memory consists of multiple levels of cache.

                        ===== Points in favor =====

Ambitious goal of full, modular verification of hardware systems.
Modularity enables reuse of models across systems.

                        ===== Points against =====

Not being well-versed in formal proofs, I personally found the paper hard to digest.

                      ===== Comments for author =====

For full disclosure, I'm not a formal verification person so I cannot judge the soundness of the proofs. I have a basic understanding of cache coherence and memory consistency models -- I'm not an expert though -- but as far as I can judge, I believe the paper is sound, and all the statements made regarding cache coherence and memory consistency appear to be correct. I have no expertise in verification of cache coherence and memory consistency models, so I cannot judge the significance of the contributions claimed in this paper.

I'm surprised that none of the work by Dan Sorin (Duke) is referenced in the paper, see http://people.ee.duke.edu/~sorin/verif-aware/ for Sorin's work on verification-aware architectures.

===========================================================================
                           PLDI '15 Review #11B
                     Updated 9 Dec 2014 4:31:06am EST
---------------------------------------------------------------------------
   Paper #11: Modular Deductive Verification of Multiprocessor Hardware
              Designs
---------------------------------------------------------------------------

                      Overall merit: 1. Reject
                         Conviction: 2. Low

                         ===== Paper summary =====

This paper presents a framework for modular verification of hardware:
components are designed in BlueSpec, and one can prove their correctness modularly. As an illustration, the paper proposes a system that provably implements SC.

                        ===== Points in favor =====

I like the direction of this paper and it's quite well written.

                        ===== Points against =====

I found no evidence that the SC system proposed in this paper is realistic. Is that even the right question to ask? I guess what I'm asking for is a sense of who would be the target audience of
this paper; who do you think would specify their system in your framework?

                      ===== Comments for author =====

More details:

page 1:

ideal memory: what is this?

ideal processor: what is this?

page 2:

compilers that convert LTSes into efficient hardware: I would expect to see a cite here

Bluespec [...] served as the model: what does that mean?

competitive performance: do you have a bibliographical reference here, or some data?

that peak beneath abstraction boundaries: what does that mean?

first machine-checked proof: did you make your development available? I'd be interested in seeing it!

We believe such a memory is also the correct abstraction for implementing relaxed memory models: that sounds very interesting; would you explain why that is?

good ideas: such as?

a simple formal model of sequential consistency: it sounds a bit like your model is going to be different from Lamport's; is that the case?

page 3:

infinite families of hardware designs, which resist reduction to finite-state models: I couldn't decide if reduction applied to 'infinite families' or 'hardware designs'; it might be worth making that clear

to be obey: to obey

the empirical challenges: which challenges?

include operational semantics: you cite Mador-Haim et al here, but that paper does not include an operational semantics, so shouldn't be there

relating the axiomatic specification to [....] operational semantics: Alglave et al at TOPLAS 14 does that too

page 4:

can be divided logically into 3 components as shown in Figure 2: is that a well-known thing? it might be worth citing something here?

page 5:

we are aware of, the main memory still exposes atomic loads and stores: would you explain what atomic means here? and what it means to expose accesses?

the properties of local buffers: why buffers only? why not caches for example?

implements sequential consistency: is your proof tied to targetting SC?

page 6:

a system refining: or equivalent?

page 9:

a transliteration: did you do this transliteration yourself? was it a big implementation effort?

page 11:

distributed shared-memory: I'm not sure I understand what you mean by
distributed here

cover finer-grained architectural details: would you give examples?

in some cases can be transliterated directly: perhaps worth a cite here, and some details as to which cases you're thinking of?

===========================================================================
                           PLDI '15 Review #11C
                     Updated 10 Dec 2014 2:35:56pm EST
---------------------------------------------------------------------------
   Paper #11: Modular Deductive Verification of Multiprocessor Hardware
              Designs
---------------------------------------------------------------------------

                      Overall merit: 1. Reject
                         Conviction: 1. High

                         ===== Paper summary =====

This paper presents a verification (within Coq) of a shared-memory
multiprocessor design, with an arbitrary hierarchy of asynchronous
caches and some speculative execution, which nonetheless provides
sequential consistency.

                        ===== Points in favor =====

A machine-checked proof of an implementation of sequential
consistency, that will (probably, not demonstrated) scale to some more
relaxed consistency models.

Standard LTS-refinement techniques applied (to my knowledge) in a novel
area.

                        ===== Points against =====

The value of this line of work really would appear when (if) it
handles relaxed memory consistency, since all commercial designs are
relaxed. I believe the approach will scale up to TSO (such as
x86-TSO), but not further to ARM/Power/Alpha etc. Even x86-TSO would
have been a good demonstration.

There are existing proofs of sequential consistency for several
variants of cache protocols, which should be discussed. (See below).

                      ===== Comments for author =====

The use of the word "atomicity" is a bit misleading. While indeed all
popular ISAs expose a memory that responds to loads and stores
atomically, many of the more relaxed designs can respond to loads from
any of several buffers (which you have), and several processor threads
can share buffers (which you don't). This can lead to so-called
non-multi-copy-atomic designs, in particular showing the well-known
Independent Reads of Independent Writes problem. Question: can you
easily handle such an extension? I rather suspect not, but would be
glad to be told otherwise.

The speculative execution is perhaps too simple, restricted to
speculating loads and then verifying answers in program order. Many
modern processors (e.g. several ARM designs) will not throw away load
answers just because the verification step produces a different value
(as all that is required is that the load *could* have produced that value).
Conversely, with several stores of the same value, merely seeing the
same value is an insufficient check for cache-coherent designs.

While the LTS formalism is intuitive and straightforward to mechanize,
would it not be equally valid to mechanize some other styles of
proofs, such as from TLA, or from "Lamport Clocks: Verifying a
Directory Cache-Coherence Protocol" (SPAA'98)? (the later of which you
should discuss). I would like a more detailed justification of the
modularization advantages of your technique.

Your reference to TLC's cache verification should cite "Checking Cache
Coherence Protocols with TLA+". Furthermore, Moore's verification
"An ACL2 proof of write invalidate cache coherence" (CAV'98) also deserves
discussion.

===========================================================================
                           PLDI '15 Review #11D
                    Updated 14 Dec 2014 10:32:07am EST
---------------------------------------------------------------------------
   Paper #11: Modular Deductive Verification of Multiprocessor Hardware
              Designs
---------------------------------------------------------------------------

                      Overall merit: 2. Accept
                         Conviction: 1. High

                         ===== Paper summary =====

The authors verify correctness of a multiprocessor hardware model.
- The correctness criterion is the safety part of sequential consistency.
- The verified architecture is a modular composition of cores that communicate with a memory cache hierarchy via an asynchronous load-store interface.

The proof methodology uses standard trace refinement and compositional verification to show that the implementation refines the spec (n-processor sequential consistency). The proof mechanics (labeled transition systems, refinement) are carefully introduced and explained, and the overall proof structure is very clear.

The paper starts with the relatively simple "top" of the abstraction:
- defining sequential consistency using an LTS.
- defining a simple abstract in-order core as an LTS.
- defining a simple atomic memory as an LTS.
- showing that the composition of the two latter refines the first.

The nontrivial parts are the two (fully independent) refinement steps for core / memory:

(core) prove that the detailed model of the core (featuring branch prediction and a ROB) refines the simple in-order core. The formal model of the core is fully included in the paper (but the ROB, ISA-specifics, and the branch predictor are abstractly represented by their functional specifications, rather than by an
implementation) and occupies about 1/2 page.

(memory) prove that the detailed model of the memory (featuring hierarchical MSI caches and a directory-based cache coherence protocol) refines the simple atomic memory.  The formal hierachical model is included in the paper and occupies about 1 page.

The detailed models are still abstract in many ways (e.g. ISA, register definitions, etc.) but are accurate representations of what is taught in computer architecture classes, and are "accurate simplifications" of real systems.

The authors say they have mechanically checked the proofs in Coq.

                        ===== Points in favor =====

- I consider the work in this paper to be a credible, useful, not-too-tiny step within the ultimate goal of building a completely verified multiprocessor system (spanning applications, compilers,  hardware). Generally speaking, this line of work may be more of research interest than of industrial significance, but I believe it is worth the effort, because it is hard (we can learn a lot from the effort), and it will improve the quality of the systems we build.

- I consider the paper to be "unusually well informed" with regards both to hardware architecture and verification methodology:

1. the model of the core and the memory represent valid architectural practices (as taught in computer architecture classes, and as used in some, but of course not all, real multiprocessor architectures, at least one of which I am familiar with from my own experience in industry)

2. the refinement methodology (trace refinement), albeit standard, is "tastefully" selected from the large number of previous work on this general topic (e.g. CSP, CCS, I/O automata), getting to the point right away, without distractions. Everything needed is right there.

The formalizations of the speculative ooo processor, and of the cache coherence protocol, have real value: they can serve as nice, precise, concise, verified references. For example, if I am considering writing a directory-based cache coherence protocol (which I may do in the context of distributed systems, for example), I can use Fig. 10 to bring the details back to mind and have good confidence of handling nasty race conditions correctly.

- How to decompose consistency questions into core and memory aspects (speculation/ROB on core side, atomic/nonatomic memory on the other side) is an important aspect that is not well known beyond the computer architecture community. I would love for this paper to serve as a reference for future work on relaxed memory models and weak consistency, for example.

Writing quality: well above average. It was pleasant to read. I have read many papers that try to do similar things but are not remotely as readable or interesting. In particular, papers often have a hard time explaining the proof structure. Here, the formulation of what was proved, and the proof structure, are
abundantly clear.

The proof structure is explained well and easily consumable by anyone with a taste for manual correctness proofs: it tells us all the important things (e.g. invariants), and only the important things (other stuff is abstracted away). I believe that the formalizations and invariants may be reusable in other projects,
in particular because they are well explained.

                        ===== Points against =====

These are not so much points against, but more of a wish list for things I would like to see in future papers.

- Proving liveness (progress). This seems nontrivial and important, in particular for the coherence protocol. I remain somewhat skeptic that I can trust this coherence protocol in a practical situation until I see more evidence that someone has very carefully thought about the progress problem.

- There is no clear mention of where the Coq proof is... is it public?

- There is not much detail on how the mechanized verification happened, or how I could reproduce that part of the work. Could I use your framework for my own verification projects?

                      ===== Comments for author =====

Comments

- The "halt" part seems uninteresting. What purpose does it serve? Perhaps I am missing something. Otherwise, I would suggest to completely remove it.

- The coherence protocol is a bit incomplete: it does not say when a parent will actually send a demote request to a child (ParentSendReq is nondeterministic). Can you explain this a bit - it seems important for progress.


Details

- Section 1, first paragraph: citation should be [Weaver and Germond], not [Weaver and Gremond]

- the bibliography could use some tightening... some entries span as much as 8 lines and contain unimportant and redundant information (urls, dois, etc)

- perhaps add a citation on prior work on automatic verification of sequential consistency (e.g. the papers by Jesse Bingham, Anne Condon, Alan Hu, Shaz Qadeer)
